{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Python Exercise 3: Using LASSO Regression to Model Fat Sales Data.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "j-qfVirLvEld",
        "colab_type": "code",
        "outputId": "97ad9a9b-5cf1-4ae1-8866-06a95abd5c9b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "import pandas as pd\n",
        "import pandas\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.linear_model import LassoLarsCV\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab import drive \n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "raqEtm09vI_y",
        "colab_type": "code",
        "outputId": "5175ff03-1bc4-4c27-9a37-a117819d7326",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 263
        }
      },
      "source": [
        "df=pd.read_csv('gdrive/My Drive/finalmaster-ratios.csv')\n",
        "df.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th># Purchases</th>\n",
              "      <th>B01001001</th>\n",
              "      <th>B01001002</th>\n",
              "      <th>B01001003</th>\n",
              "      <th>B01001004</th>\n",
              "      <th>B01001005</th>\n",
              "      <th>B01001006</th>\n",
              "      <th>B01001007</th>\n",
              "      <th>B01001008</th>\n",
              "      <th>B01001009</th>\n",
              "      <th>B01001010</th>\n",
              "      <th>B01001011</th>\n",
              "      <th>B01001012</th>\n",
              "      <th>B01001013</th>\n",
              "      <th>B01001014</th>\n",
              "      <th>B01001015</th>\n",
              "      <th>B01001016</th>\n",
              "      <th>B01001017</th>\n",
              "      <th>B01001018</th>\n",
              "      <th>B01001019</th>\n",
              "      <th>B01001020</th>\n",
              "      <th>B01001021</th>\n",
              "      <th>B01001022</th>\n",
              "      <th>B01001023</th>\n",
              "      <th>B01001024</th>\n",
              "      <th>B01001025</th>\n",
              "      <th>B01001026</th>\n",
              "      <th>B01001027</th>\n",
              "      <th>B01001028</th>\n",
              "      <th>B01001029</th>\n",
              "      <th>B01001030</th>\n",
              "      <th>B01001031</th>\n",
              "      <th>B01001032</th>\n",
              "      <th>B01001033</th>\n",
              "      <th>B01001034</th>\n",
              "      <th>B01001035</th>\n",
              "      <th>B01001036</th>\n",
              "      <th>B01001037</th>\n",
              "      <th>B01001038</th>\n",
              "      <th>B01001039</th>\n",
              "      <th>...</th>\n",
              "      <th>B15002013</th>\n",
              "      <th>B15002014</th>\n",
              "      <th>B15002015</th>\n",
              "      <th>B15002016</th>\n",
              "      <th>B15002017</th>\n",
              "      <th>B15002018</th>\n",
              "      <th>B15002019</th>\n",
              "      <th>B15002020</th>\n",
              "      <th>B15002021</th>\n",
              "      <th>B15002022</th>\n",
              "      <th>B15002023</th>\n",
              "      <th>B15002024</th>\n",
              "      <th>B15002025</th>\n",
              "      <th>B15002026</th>\n",
              "      <th>B15002027</th>\n",
              "      <th>B15002028</th>\n",
              "      <th>B15002029</th>\n",
              "      <th>B15002030</th>\n",
              "      <th>B15002031</th>\n",
              "      <th>B15002032</th>\n",
              "      <th>B15002033</th>\n",
              "      <th>B15002034</th>\n",
              "      <th>B15002035</th>\n",
              "      <th>B19001001</th>\n",
              "      <th>B19001002</th>\n",
              "      <th>B19001003</th>\n",
              "      <th>B19001004</th>\n",
              "      <th>B19001005</th>\n",
              "      <th>B19001006</th>\n",
              "      <th>B19001007</th>\n",
              "      <th>B19001008</th>\n",
              "      <th>B19001009</th>\n",
              "      <th>B19001010</th>\n",
              "      <th>B19001011</th>\n",
              "      <th>B19001012</th>\n",
              "      <th>B19001013</th>\n",
              "      <th>B19001014</th>\n",
              "      <th>B19001015</th>\n",
              "      <th>B19001016</th>\n",
              "      <th>B19001017</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>22</td>\n",
              "      <td>206252</td>\n",
              "      <td>469.226965</td>\n",
              "      <td>31.432422</td>\n",
              "      <td>35.219052</td>\n",
              "      <td>33.628765</td>\n",
              "      <td>20.121017</td>\n",
              "      <td>12.610787</td>\n",
              "      <td>6.734480</td>\n",
              "      <td>6.225394</td>\n",
              "      <td>19.432539</td>\n",
              "      <td>28.101546</td>\n",
              "      <td>28.421543</td>\n",
              "      <td>26.390047</td>\n",
              "      <td>31.989993</td>\n",
              "      <td>31.359696</td>\n",
              "      <td>32.116052</td>\n",
              "      <td>32.213021</td>\n",
              "      <td>12.184124</td>\n",
              "      <td>18.361034</td>\n",
              "      <td>9.454454</td>\n",
              "      <td>15.175610</td>\n",
              "      <td>16.281054</td>\n",
              "      <td>11.025348</td>\n",
              "      <td>6.230243</td>\n",
              "      <td>4.518744</td>\n",
              "      <td>530.773035</td>\n",
              "      <td>31.999690</td>\n",
              "      <td>34.322091</td>\n",
              "      <td>32.649380</td>\n",
              "      <td>20.101623</td>\n",
              "      <td>12.513818</td>\n",
              "      <td>8.072649</td>\n",
              "      <td>6.021760</td>\n",
              "      <td>22.923414</td>\n",
              "      <td>31.335454</td>\n",
              "      <td>31.558482</td>\n",
              "      <td>31.063941</td>\n",
              "      <td>36.082074</td>\n",
              "      <td>34.845723</td>\n",
              "      <td>...</td>\n",
              "      <td>64.610300</td>\n",
              "      <td>31.449746</td>\n",
              "      <td>58.735313</td>\n",
              "      <td>20.071053</td>\n",
              "      <td>6.726751</td>\n",
              "      <td>5.882267</td>\n",
              "      <td>543.803963</td>\n",
              "      <td>6.974272</td>\n",
              "      <td>2.504332</td>\n",
              "      <td>5.904107</td>\n",
              "      <td>11.917415</td>\n",
              "      <td>10.767170</td>\n",
              "      <td>18.141844</td>\n",
              "      <td>19.779852</td>\n",
              "      <td>10.956451</td>\n",
              "      <td>181.418442</td>\n",
              "      <td>26.717724</td>\n",
              "      <td>85.271036</td>\n",
              "      <td>54.243532</td>\n",
              "      <td>72.647457</td>\n",
              "      <td>30.816383</td>\n",
              "      <td>2.831933</td>\n",
              "      <td>2.912014</td>\n",
              "      <td>1000</td>\n",
              "      <td>105.667996</td>\n",
              "      <td>82.298375</td>\n",
              "      <td>68.141163</td>\n",
              "      <td>67.336195</td>\n",
              "      <td>63.566902</td>\n",
              "      <td>59.439845</td>\n",
              "      <td>49.409690</td>\n",
              "      <td>53.306757</td>\n",
              "      <td>42.318307</td>\n",
              "      <td>83.167229</td>\n",
              "      <td>89.249208</td>\n",
              "      <td>102.141470</td>\n",
              "      <td>52.872330</td>\n",
              "      <td>36.440765</td>\n",
              "      <td>23.446284</td>\n",
              "      <td>21.197485</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>7</td>\n",
              "      <td>61399</td>\n",
              "      <td>486.538869</td>\n",
              "      <td>22.899396</td>\n",
              "      <td>21.531295</td>\n",
              "      <td>27.036271</td>\n",
              "      <td>16.808091</td>\n",
              "      <td>28.355511</td>\n",
              "      <td>18.192479</td>\n",
              "      <td>13.534422</td>\n",
              "      <td>21.466148</td>\n",
              "      <td>24.886399</td>\n",
              "      <td>23.534585</td>\n",
              "      <td>21.319565</td>\n",
              "      <td>27.101419</td>\n",
              "      <td>30.961416</td>\n",
              "      <td>37.117868</td>\n",
              "      <td>36.466392</td>\n",
              "      <td>12.557208</td>\n",
              "      <td>20.554081</td>\n",
              "      <td>12.182609</td>\n",
              "      <td>15.651721</td>\n",
              "      <td>20.668089</td>\n",
              "      <td>15.961172</td>\n",
              "      <td>10.423623</td>\n",
              "      <td>7.329110</td>\n",
              "      <td>513.461131</td>\n",
              "      <td>18.974250</td>\n",
              "      <td>23.404290</td>\n",
              "      <td>23.892897</td>\n",
              "      <td>17.036108</td>\n",
              "      <td>35.310021</td>\n",
              "      <td>18.534504</td>\n",
              "      <td>17.101256</td>\n",
              "      <td>22.785387</td>\n",
              "      <td>22.150198</td>\n",
              "      <td>22.622518</td>\n",
              "      <td>21.303279</td>\n",
              "      <td>26.971123</td>\n",
              "      <td>32.329517</td>\n",
              "      <td>...</td>\n",
              "      <td>56.929829</td>\n",
              "      <td>46.381727</td>\n",
              "      <td>65.707446</td>\n",
              "      <td>35.509451</td>\n",
              "      <td>16.782205</td>\n",
              "      <td>9.201536</td>\n",
              "      <td>515.086529</td>\n",
              "      <td>3.017306</td>\n",
              "      <td>1.047329</td>\n",
              "      <td>1.371503</td>\n",
              "      <td>6.358785</td>\n",
              "      <td>4.937410</td>\n",
              "      <td>8.303825</td>\n",
              "      <td>9.700264</td>\n",
              "      <td>7.555733</td>\n",
              "      <td>174.155902</td>\n",
              "      <td>25.834123</td>\n",
              "      <td>60.146626</td>\n",
              "      <td>62.440776</td>\n",
              "      <td>76.604658</td>\n",
              "      <td>55.383771</td>\n",
              "      <td>8.977108</td>\n",
              "      <td>9.251409</td>\n",
              "      <td>1000</td>\n",
              "      <td>71.289558</td>\n",
              "      <td>59.062447</td>\n",
              "      <td>54.704688</td>\n",
              "      <td>60.966323</td>\n",
              "      <td>53.012354</td>\n",
              "      <td>60.881706</td>\n",
              "      <td>59.231680</td>\n",
              "      <td>50.093078</td>\n",
              "      <td>40.700626</td>\n",
              "      <td>92.612963</td>\n",
              "      <td>117.363344</td>\n",
              "      <td>113.344051</td>\n",
              "      <td>75.774243</td>\n",
              "      <td>33.000508</td>\n",
              "      <td>33.169741</td>\n",
              "      <td>24.792689</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>73170</td>\n",
              "      <td>489.859232</td>\n",
              "      <td>28.905289</td>\n",
              "      <td>36.271696</td>\n",
              "      <td>28.235616</td>\n",
              "      <td>21.566216</td>\n",
              "      <td>12.218122</td>\n",
              "      <td>7.243406</td>\n",
              "      <td>7.380074</td>\n",
              "      <td>16.933169</td>\n",
              "      <td>24.914582</td>\n",
              "      <td>26.896269</td>\n",
              "      <td>31.802651</td>\n",
              "      <td>30.531639</td>\n",
              "      <td>36.258029</td>\n",
              "      <td>35.998360</td>\n",
              "      <td>33.429001</td>\n",
              "      <td>13.625803</td>\n",
              "      <td>19.406861</td>\n",
              "      <td>12.245456</td>\n",
              "      <td>14.664480</td>\n",
              "      <td>21.169878</td>\n",
              "      <td>15.293153</td>\n",
              "      <td>8.610086</td>\n",
              "      <td>6.259396</td>\n",
              "      <td>510.140768</td>\n",
              "      <td>26.171928</td>\n",
              "      <td>30.681973</td>\n",
              "      <td>31.925653</td>\n",
              "      <td>19.789531</td>\n",
              "      <td>10.072434</td>\n",
              "      <td>5.056717</td>\n",
              "      <td>6.218396</td>\n",
              "      <td>15.757824</td>\n",
              "      <td>24.449911</td>\n",
              "      <td>26.595599</td>\n",
              "      <td>27.210605</td>\n",
              "      <td>37.556376</td>\n",
              "      <td>37.050704</td>\n",
              "      <td>...</td>\n",
              "      <td>54.602613</td>\n",
              "      <td>40.613027</td>\n",
              "      <td>43.363788</td>\n",
              "      <td>12.280185</td>\n",
              "      <td>5.796247</td>\n",
              "      <td>3.438452</td>\n",
              "      <td>523.980745</td>\n",
              "      <td>5.422930</td>\n",
              "      <td>4.224384</td>\n",
              "      <td>11.828274</td>\n",
              "      <td>18.331860</td>\n",
              "      <td>15.089891</td>\n",
              "      <td>21.731015</td>\n",
              "      <td>18.685529</td>\n",
              "      <td>7.014441</td>\n",
              "      <td>155.241183</td>\n",
              "      <td>45.466156</td>\n",
              "      <td>71.185775</td>\n",
              "      <td>65.802142</td>\n",
              "      <td>56.272718</td>\n",
              "      <td>24.580018</td>\n",
              "      <td>1.689753</td>\n",
              "      <td>1.414677</td>\n",
              "      <td>1000</td>\n",
              "      <td>102.538696</td>\n",
              "      <td>82.960331</td>\n",
              "      <td>74.828305</td>\n",
              "      <td>79.133495</td>\n",
              "      <td>66.081252</td>\n",
              "      <td>78.245122</td>\n",
              "      <td>63.996993</td>\n",
              "      <td>47.322923</td>\n",
              "      <td>42.505211</td>\n",
              "      <td>70.420610</td>\n",
              "      <td>90.033143</td>\n",
              "      <td>98.677692</td>\n",
              "      <td>54.703249</td>\n",
              "      <td>20.125056</td>\n",
              "      <td>11.890525</td>\n",
              "      <td>16.537397</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>94</td>\n",
              "      <td>251724</td>\n",
              "      <td>505.585483</td>\n",
              "      <td>32.054949</td>\n",
              "      <td>31.757004</td>\n",
              "      <td>28.102207</td>\n",
              "      <td>18.651380</td>\n",
              "      <td>12.080692</td>\n",
              "      <td>7.035483</td>\n",
              "      <td>7.686991</td>\n",
              "      <td>25.790151</td>\n",
              "      <td>42.129475</td>\n",
              "      <td>35.824951</td>\n",
              "      <td>32.058922</td>\n",
              "      <td>27.677138</td>\n",
              "      <td>33.842621</td>\n",
              "      <td>38.176733</td>\n",
              "      <td>32.722347</td>\n",
              "      <td>12.493842</td>\n",
              "      <td>16.394940</td>\n",
              "      <td>11.504664</td>\n",
              "      <td>15.914255</td>\n",
              "      <td>16.394940</td>\n",
              "      <td>13.196994</td>\n",
              "      <td>8.648361</td>\n",
              "      <td>5.446441</td>\n",
              "      <td>494.414517</td>\n",
              "      <td>33.123580</td>\n",
              "      <td>28.082344</td>\n",
              "      <td>30.171934</td>\n",
              "      <td>16.863708</td>\n",
              "      <td>9.280005</td>\n",
              "      <td>5.390825</td>\n",
              "      <td>5.609318</td>\n",
              "      <td>19.453846</td>\n",
              "      <td>35.614403</td>\n",
              "      <td>32.082757</td>\n",
              "      <td>28.809331</td>\n",
              "      <td>27.911522</td>\n",
              "      <td>32.690566</td>\n",
              "      <td>...</td>\n",
              "      <td>88.227492</td>\n",
              "      <td>44.076261</td>\n",
              "      <td>87.939148</td>\n",
              "      <td>44.404973</td>\n",
              "      <td>9.671057</td>\n",
              "      <td>7.283569</td>\n",
              "      <td>502.912274</td>\n",
              "      <td>4.509700</td>\n",
              "      <td>0.980370</td>\n",
              "      <td>3.552398</td>\n",
              "      <td>5.986021</td>\n",
              "      <td>7.398907</td>\n",
              "      <td>9.740260</td>\n",
              "      <td>10.605292</td>\n",
              "      <td>7.485410</td>\n",
              "      <td>141.242417</td>\n",
              "      <td>43.078591</td>\n",
              "      <td>84.479020</td>\n",
              "      <td>52.069156</td>\n",
              "      <td>89.836451</td>\n",
              "      <td>33.932320</td>\n",
              "      <td>4.129086</td>\n",
              "      <td>3.886877</td>\n",
              "      <td>1000</td>\n",
              "      <td>61.632139</td>\n",
              "      <td>46.526521</td>\n",
              "      <td>48.437595</td>\n",
              "      <td>54.221644</td>\n",
              "      <td>51.680322</td>\n",
              "      <td>60.066684</td>\n",
              "      <td>54.790900</td>\n",
              "      <td>48.681562</td>\n",
              "      <td>43.873381</td>\n",
              "      <td>84.717507</td>\n",
              "      <td>112.204444</td>\n",
              "      <td>127.137252</td>\n",
              "      <td>83.019904</td>\n",
              "      <td>43.731067</td>\n",
              "      <td>38.851729</td>\n",
              "      <td>40.427349</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>37382</td>\n",
              "      <td>495.586111</td>\n",
              "      <td>25.413301</td>\n",
              "      <td>29.318924</td>\n",
              "      <td>26.162324</td>\n",
              "      <td>19.260607</td>\n",
              "      <td>12.893906</td>\n",
              "      <td>6.580707</td>\n",
              "      <td>7.062222</td>\n",
              "      <td>17.334546</td>\n",
              "      <td>32.930287</td>\n",
              "      <td>28.302392</td>\n",
              "      <td>28.569900</td>\n",
              "      <td>26.804344</td>\n",
              "      <td>30.549462</td>\n",
              "      <td>36.595153</td>\n",
              "      <td>42.373335</td>\n",
              "      <td>16.398267</td>\n",
              "      <td>22.871970</td>\n",
              "      <td>17.174041</td>\n",
              "      <td>15.221229</td>\n",
              "      <td>23.433738</td>\n",
              "      <td>14.391953</td>\n",
              "      <td>7.383233</td>\n",
              "      <td>8.560270</td>\n",
              "      <td>504.413889</td>\n",
              "      <td>26.563587</td>\n",
              "      <td>30.255203</td>\n",
              "      <td>24.798031</td>\n",
              "      <td>16.237761</td>\n",
              "      <td>11.101600</td>\n",
              "      <td>4.788401</td>\n",
              "      <td>5.189663</td>\n",
              "      <td>17.842812</td>\n",
              "      <td>30.014445</td>\n",
              "      <td>27.767375</td>\n",
              "      <td>30.763469</td>\n",
              "      <td>25.199294</td>\n",
              "      <td>29.613183</td>\n",
              "      <td>...</td>\n",
              "      <td>102.957039</td>\n",
              "      <td>36.711921</td>\n",
              "      <td>70.039055</td>\n",
              "      <td>33.587502</td>\n",
              "      <td>5.021387</td>\n",
              "      <td>5.244560</td>\n",
              "      <td>511.177236</td>\n",
              "      <td>2.045750</td>\n",
              "      <td>3.236005</td>\n",
              "      <td>1.525014</td>\n",
              "      <td>7.476288</td>\n",
              "      <td>4.314674</td>\n",
              "      <td>8.554956</td>\n",
              "      <td>13.204389</td>\n",
              "      <td>7.773852</td>\n",
              "      <td>130.890831</td>\n",
              "      <td>53.784638</td>\n",
              "      <td>99.311884</td>\n",
              "      <td>57.095034</td>\n",
              "      <td>76.027525</td>\n",
              "      <td>38.422912</td>\n",
              "      <td>4.351869</td>\n",
              "      <td>3.161614</td>\n",
              "      <td>1000</td>\n",
              "      <td>51.125525</td>\n",
              "      <td>58.438255</td>\n",
              "      <td>68.930434</td>\n",
              "      <td>74.717029</td>\n",
              "      <td>63.970495</td>\n",
              "      <td>59.710034</td>\n",
              "      <td>58.883378</td>\n",
              "      <td>51.761414</td>\n",
              "      <td>47.310187</td>\n",
              "      <td>81.902582</td>\n",
              "      <td>93.793717</td>\n",
              "      <td>130.103014</td>\n",
              "      <td>71.982704</td>\n",
              "      <td>36.118530</td>\n",
              "      <td>31.603714</td>\n",
              "      <td>19.648989</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows Ã— 190 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   # Purchases  B01001001   B01001002  ...  B19001015  B19001016  B19001017\n",
              "0           22     206252  469.226965  ...  36.440765  23.446284  21.197485\n",
              "1            7      61399  486.538869  ...  33.000508  33.169741  24.792689\n",
              "2            3      73170  489.859232  ...  20.125056  11.890525  16.537397\n",
              "3           94     251724  505.585483  ...  43.731067  38.851729  40.427349\n",
              "4            0      37382  495.586111  ...  36.118530  31.603714  19.648989\n",
              "\n",
              "[5 rows x 190 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xwZeB-zNxVzg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create a list of all the predictors you're going to feed into the LassoLarsCV model\n",
        "allvariablenames = list(df.columns.values)\n",
        "listofallpredictors = allvariablenames[8:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FAQ0MiZRxy-k",
        "colab_type": "code",
        "outputId": "ecca2682-648a-44f4-ab27-5963035c51b2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 116
        }
      },
      "source": [
        "#load predictors into dataframe\n",
        "predictors = df[listofallpredictors]  \n",
        "#load target into dataframe\n",
        "target = df['# Purchases']   \n",
        "target.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    22\n",
              "1     7\n",
              "2     3\n",
              "3    94\n",
              "4     0\n",
              "Name: # Purchases, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GLJK3yuLyy7X",
        "colab_type": "code",
        "outputId": "4c29c260-be16-414a-f870-9745e4848c62",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# split data into train and test sets, with 30% retained for test\n",
        "pred_train, pred_test, tar_train, tar_test = train_test_split(predictors, target, test_size=.3, random_state=123) \n",
        "#  (1) pred_train, the predictors training set \n",
        "#  (2) pred_test, the predictors test test \n",
        "#  (3) tar_train, the target training set \n",
        "#  (4) tar_test, the target test set.\n",
        "model = LassoLarsCV(cv=10, precompute=False).fit(pred_train, tar_train)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=1.496e+00, with an active set of 5 regressors, and the smallest cholesky pivot element being 6.322e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=1.098e+00, with an active set of 8 regressors, and the smallest cholesky pivot element being 6.322e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=7.329e-01, with an active set of 10 regressors, and the smallest cholesky pivot element being 6.322e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=6.051e-01, with an active set of 11 regressors, and the smallest cholesky pivot element being 6.664e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:604: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 17 iterations, alpha=5.739e-01, previous alpha=5.739e-01, with an active set of 14 regressors.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=4.100e-01, with an active set of 16 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=2.867e-01, with an active set of 27 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 44 iterations, i.e. alpha=2.050e-01, with an active set of 36 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 45 iterations, i.e. alpha=2.013e-01, with an active set of 37 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:604: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 46 iterations, alpha=1.986e-01, previous alpha=1.960e-01, with an active set of 37 regressors.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=1.365e+00, with an active set of 4 regressors, and the smallest cholesky pivot element being 6.053e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.008e+00, with an active set of 7 regressors, and the smallest cholesky pivot element being 6.053e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=8.144e-01, with an active set of 9 regressors, and the smallest cholesky pivot element being 6.053e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=6.642e-01, with an active set of 10 regressors, and the smallest cholesky pivot element being 6.053e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=5.644e-01, with an active set of 13 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:604: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 16 iterations, alpha=5.626e-01, previous alpha=5.354e-01, with an active set of 13 regressors.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=1.439e+00, with an active set of 4 regressors, and the smallest cholesky pivot element being 3.799e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=1.037e+00, with an active set of 9 regressors, and the smallest cholesky pivot element being 3.799e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:604: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 15 iterations, alpha=7.201e-01, previous alpha=7.200e-01, with an active set of 12 regressors.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 106 iterations, i.e. alpha=4.546e-02, with an active set of 90 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 107 iterations, i.e. alpha=4.539e-02, with an active set of 91 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:604: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 116 iterations, alpha=3.652e-02, previous alpha=3.599e-02, with an active set of 99 regressors.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=1.572e+00, with an active set of 4 regressors, and the smallest cholesky pivot element being 3.942e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=1.132e+00, with an active set of 10 regressors, and the smallest cholesky pivot element being 3.942e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=7.640e-01, with an active set of 12 regressors, and the smallest cholesky pivot element being 3.942e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 26 iterations, i.e. alpha=3.863e-01, with an active set of 24 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 26 iterations, i.e. alpha=3.863e-01, with an active set of 24 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=3.560e-01, with an active set of 24 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 54 iterations, i.e. alpha=1.932e-01, with an active set of 36 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 59 iterations, i.e. alpha=1.737e-01, with an active set of 41 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:604: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 64 iterations, alpha=1.702e-01, previous alpha=1.687e-01, with an active set of 45 regressors.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=1.644e+00, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.178e+00, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=9.316e-01, with an active set of 9 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=8.155e-01, with an active set of 10 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:604: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 32 iterations, alpha=3.203e-01, previous alpha=3.135e-01, with an active set of 25 regressors.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 23 iterations, i.e. alpha=2.100e-01, with an active set of 23 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 66 iterations, i.e. alpha=6.523e-02, with an active set of 60 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 66 iterations, i.e. alpha=6.523e-02, with an active set of 60 regressors, and the smallest cholesky pivot element being 6.495e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:604: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 68 iterations, alpha=6.435e-02, previous alpha=6.382e-02, with an active set of 61 regressors.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=1.147e+00, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=8.014e-01, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=8.010e-01, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=5.693e-01, with an active set of 10 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=5.099e-01, with an active set of 11 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 19 iterations, i.e. alpha=3.948e-01, with an active set of 17 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:604: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 32 iterations, alpha=3.137e-01, previous alpha=3.116e-01, with an active set of 27 regressors.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 11 iterations, i.e. alpha=5.644e-01, with an active set of 11 regressors, and the smallest cholesky pivot element being 9.064e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 25 iterations, i.e. alpha=2.822e-01, with an active set of 23 regressors, and the smallest cholesky pivot element being 9.125e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:604: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 37 iterations, alpha=2.062e-01, previous alpha=1.984e-01, with an active set of 34 regressors.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=1.372e+00, with an active set of 4 regressors, and the smallest cholesky pivot element being 6.989e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=9.825e-01, with an active set of 8 regressors, and the smallest cholesky pivot element being 6.989e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=6.732e-01, with an active set of 10 regressors, and the smallest cholesky pivot element being 6.989e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 14 iterations, i.e. alpha=5.285e-01, with an active set of 12 regressors, and the smallest cholesky pivot element being 7.300e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:604: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 16 iterations, alpha=5.186e-01, previous alpha=5.185e-01, with an active set of 15 regressors.\n",
            "  ConvergenceWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "06dpJRygza8Y",
        "colab_type": "text"
      },
      "source": [
        "## Question 1:In your own words, explain what the above lines of code are doing.Why am I doing it? Explain each line."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lYgLop8sy3p_",
        "colab_type": "code",
        "outputId": "b06b15df-062a-4fb3-d32f-8854b57d3af6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 166
        }
      },
      "source": [
        "# build coefficent chart\n",
        "# Create a predictors model data frame named 'predictors' that contains the list of all possible predictor variables (load all predictors to this model)\n",
        "predictors_model=pd.DataFrame(listofallpredictors)\n",
        "# named the column labels of predictors_model to 'label'\n",
        "predictors_model.columns = ['label']\n",
        "# add a new column called 'coeff' and append all coefficents from regression model.\n",
        "predictors_model['coeff'] = model.coef_\n",
        "# check all the index and row in predictors\n",
        "for index, row in predictors_model.iterrows():\n",
        "    # for loop that go through predictor_model table and print out the coefficent with name that larger than zero.\n",
        "    if row['coeff'] > 0:\n",
        "        # print out the significant values\n",
        "        print(row.values)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['B01001014' 0.8558761066941788]\n",
            "['B01001036' 2.5053482381631653]\n",
            "['B01001037' 0.8892493223320962]\n",
            "['B01001038' 1.5316387928880384]\n",
            "['B02001005' 0.41252295298457853]\n",
            "['B13014026' 0.48004105312075906]\n",
            "['B13014027' 0.6978957445987839]\n",
            "['B13016001' 875149895.329212]\n",
            "['B19001017' 1.4834348068681533]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X2Qnstyb3MJR",
        "colab_type": "text"
      },
      "source": [
        "## Question 2: Interpret each variable intuitively. What Census variables most predict sales? What does that mean, practically? Here's what I'd put for the example above. \"In areas where there are more females aged 30-34, we sell more Bobo Bars.\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-EDRXdMH29eK",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "['B01001014' 0.8557908775529921] Males aged 40 to 44 Years.\n",
        "In areas where there are more males aged 40-44, we sell 0.8557908775529921 unit more Bobo Bars\n",
        "\n",
        "['B01001036' 2.505392496591849] Females aged 30 to 34 Years.\n",
        "In areas where there are more females aged 30-34, we sell 2.505392496591849 unit more Bobo Bars\n",
        "\n",
        "['B01001037' 0.8894214357013622] Females aged 35 to 39 Years.\n",
        "In areas where there are more females aged 35-39, we sell 0.8894214357013622] unit more Bobo Bars\n",
        "\n",
        "['B01001038' 1.5315839680821497] Females aged 40 to 44 Years.\n",
        "In areas where there are more females aged 40-44, we sell 1.5315839680821497 unit more Bobo Bars\n",
        "\n",
        "['B02001005' 0.4125408937426837] Asian Alone.\n",
        "In areas where there are asian alone, we sell 0.4125408937426837 unit more Bobo Bars\n",
        "\n",
        "['B13014026' 0.4800240326923769] Women 15 to 50 Years Who Had a Birth in the Past 12 Months with Bachelor's Degree.\n",
        "In areas where there are more females aged 15-50 that had a birth in the past 12 months with Bachelor's degree, we sell 0.4800240326923769 unit more Bobo Bars\n",
        "\n",
        "['B13014027' 0.6977454940063235] Women 15 to 50 Years Who Had a Birth in the Past 12 Months with Graduate or Professional Degree.\n",
        "In areas where there are more females aged 15-50 that had a birth in the past 12 months with graduate or professional degree, we sell 0.6977454940063235 unit more Bobo Bars\n",
        "\n",
        "['B13016001' 874922971.7249781] Women 15 to 50 Years Who Had a Birth in the Past 12 Months.\n",
        "In areas where there are more females aged 15-50 that had a birth in the past 12 months, we sell 874922971.7249781 unit more Bobo Bars\n",
        "\n",
        "['B19001017' 1.4834465563617387] Household with income 200,000 or More.\n",
        "In areas where there are more household with income 200,000 or more, we sell 1.4834465563617387 unit more Bobo Bars"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HMZMBrGk429t",
        "colab_type": "text"
      },
      "source": [
        "## Question 3: If I had to report only two census variables to my boss that most steeply predicted sales, what would those be?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xn2r_ooh5H3y",
        "colab_type": "text"
      },
      "source": [
        "Since the higher the value of the coefficient, the steeper the relationship between sales and that variable.\n",
        "I will report B01001036 which is Females aged 30 to 34 Years, \n",
        "and  B13016001 which is Women 15 to 50 Years Who Had a Birth in the Past 12 Months because these 2 census have the most 2 highest value of the coefficient which means the most steep relation between sales and attributes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3hwvsa_g8_uD",
        "colab_type": "text"
      },
      "source": [
        "## Question 4:Are the training and text set mean squared errors similar? What does that mean practically?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ZxJWd8Q42Qr",
        "colab_type": "code",
        "outputId": "a7f10880-b6bf-4ba6-a026-3cbe22110229",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "train_error = mean_squared_error(tar_train, model.predict(pred_train))\n",
        "print ('training data MSE')\n",
        "print(train_error)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "training data MSE\n",
            "22025.491066757\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y0qLvGbG8WQP",
        "colab_type": "code",
        "outputId": "bc5e0753-7c81-42b3-d5c4-997fcf2b5e58",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "train_error_test = mean_squared_error(tar_test, model.predict(pred_test))\n",
        "print ('testing data MSE')\n",
        "print(train_error_test)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "testing data MSE\n",
            "41549.54803776253\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TqioVT3R8cb6",
        "colab_type": "text"
      },
      "source": [
        "The mean squared error of training data set is 22025.491066757\n",
        "and the mean squared error of testing data is 41549.54803776253 which are totaly different. The MEAN SQUARED ERROR is a measure of how close a fitted line is to data points,which means smaller mean squared error represents the closer fit to the data or model. \n",
        "From our example, the training data has smaller mean squard error which means fitting batter for our model because our model based on training data, so we also can say the model we builded might not fit testing data because MSE of testing  data is much larger which is make sense. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EiVO8pQO8f9q",
        "colab_type": "text"
      },
      "source": [
        "### Then we do R squre"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YL1fArNk8klS",
        "colab_type": "code",
        "outputId": "75e0b548-c6f6-44f3-d8d2-2f00a4dc873a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "#r squared\n",
        "rsquared_train=model.score(pred_train,tar_train)\n",
        "print ('training data R-square')\n",
        "print(rsquared_train)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "training data R-square\n",
            "0.2400221219784492\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pmf6ygXo8xRT",
        "colab_type": "code",
        "outputId": "3b135d77-c7c2-45f0-d981-c515bd9d7af5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "#r squared\n",
        "rsquared_test=model.score(pred_test,tar_test)\n",
        "print ('testing data R-square')\n",
        "print(rsquared_test)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "testing data R-square\n",
            "0.1758628512005107\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-YenlDL583b6",
        "colab_type": "text"
      },
      "source": [
        "R-squared is a statistical measure that represents the proportion of the variance for a dependent variable that's explained by an independent variable or variables in a regression model. Whereas correlation explains the strength of the relationship between an independent and dependent variable, R-squared explains to what extent the variance of one variable explains the variance of the second variable.\n",
        "So lagerer R square represents the closer fit to the data or model. From our example,The R-square of traninig data is 0.2400221219784492 and R-square of testing data is 0.1758628512005107, The training data is a better regression model as it has larger R-square value according to we build modle based on trainong data which is make sense.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0o994N4T9JkY",
        "colab_type": "text"
      },
      "source": [
        "## Question 5: If your boss asked, \"How well does Census data, overall, predict sales?\" What would you say? Why?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xg94GrWQ9WOz",
        "colab_type": "text"
      },
      "source": [
        "I would say census is not a good data for predicting sales.\n",
        "From our processing for meaning squared error and R-squard, we found both of training data are closer fitting to the model because we built model cased on training data. The problem is we got a pretty small R-square value and a large meaning squard error value for both data (training data and testing data) which means not sufficient enough to make proper prediction. We can find the sd of the regression modelâ€™s errors is only about 1/10 the size of the sd of the errors that you would get with a constant-only model. Because we only got R- square value around 20% which means the variance of its errors is 20% less than the variance of the dependent variable and the sd of its error is 11% less than the sd of the dependent variable.At the same time, the value of MSE are pretty large, so its not good.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jWLP6Jls9a-h",
        "colab_type": "text"
      },
      "source": [
        "## Question 6: What is our baseline sales number? What does that mean, practically? "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5-BSIF8M9iqs",
        "colab_type": "code",
        "outputId": "03dca68c-e422-49a6-c04b-51d01c75bff2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "print(\"y interecept:\")\n",
        "print(model.intercept_)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "y interecept:\n",
            "22.19738813257551\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tWGeXOMU9ldz",
        "colab_type": "text"
      },
      "source": [
        " In this case, we could know that the baseline sales number is 22.194675187960804,because when X=0, the corresponding y-value is the y-intercept which is 22.19738813257551. which means when variables that truly predict unique amounts of sales equal to 0, the baseline sales are 22.194675187960804."
      ]
    }
  ]
}